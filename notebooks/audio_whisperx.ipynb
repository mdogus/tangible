{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\Documents\\Python Projects\\tangible\\venv_whisperx\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\Documents\\Python Projects\\tangible\n"
     ]
    }
   ],
   "source": [
    "import whisperx\n",
    "import torch\n",
    "from docx import Document\n",
    "import os\n",
    "\n",
    "# Çalışma dizinini tangible klasörüne ayarla\n",
    "os.chdir('..')\n",
    "\n",
    "# Şu anki çalışma dizinini kontrol et\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cihazı ve uygun compute_type'ı belirle\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
    "print(f\"Cihaz: {device}, Compute Type: {compute_type}\")\n",
    "\n",
    "# WhisperX modelini yükle\n",
    "print(\"WhisperX modeli yükleniyor...\")\n",
    "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
    "print(\"WhisperX modeli yüklendi.\")\n",
    "\n",
    "# 'data' klasöründeki ses dosyasının yolunu belirtin\n",
    "audio_file = os.path.join(\"data\", \"Yönetici_1 Erdem Erdoğdu2.wav\")\n",
    "print(f\"Ses dosyası: {audio_file}\")\n",
    "\n",
    "# Ses dosyasını transkribe et\n",
    "print(\"Ses dosyası transkribe ediliyor...\")\n",
    "result = model.transcribe(audio_file, language='tr')\n",
    "print(\"Transkripsiyon tamamlandı.\")\n",
    "\n",
    "# Zaman hizalaması için model ve metadata yükle\n",
    "print(\"Zaman hizalama modeli yükleniyor...\")\n",
    "model_a, metadata = whisperx.load_align_model(language_code='tr', device=device)\n",
    "print(\"Zaman hizalama modeli yüklendi.\")\n",
    "\n",
    "# Zaman hizalaması yap\n",
    "print(\"Zaman hizalaması yapılıyor...\")\n",
    "result_aligned = whisperx.align(result[\"segments\"], model_a, metadata, audio_file, device)\n",
    "print(\"Zaman hizalaması tamamlandı.\")\n",
    "\n",
    "# Konuşmacı ayrımı için pyannote.audio tabanlı modeli yükle\n",
    "print(\"Konuşmacı ayrımı modeli yükleniyor...\")\n",
    "diarize_model = whisperx.DiarizationPipeline(use_auth_token=\"YOUR_HUGGINGFACE_TOKEN\", device=device)\n",
    "print(\"Konuşmacı ayrımı modeli yüklendi.\")\n",
    "\n",
    "# Konuşmacı ayrımı sonuçlarını al\n",
    "print(\"Konuşmacı ayrımı yapılıyor...\")\n",
    "diarize_segments = diarize_model(audio_file)\n",
    "print(\"Konuşmacı ayrımı tamamlandı.\")\n",
    "\n",
    "# Konuşmacı bilgilerini transkripsiyonla birleştir\n",
    "print(\"Konuşmacı bilgileri transkripsiyonla birleştiriliyor...\")\n",
    "result_segments, word_segments = whisperx.assign_word_speakers(diarize_segments, result_aligned)\n",
    "print(\"Birleştirme tamamlandı.\")\n",
    "\n",
    "# Sonuçları bir Word belgesine kaydet\n",
    "print(\"Sonuçlar Word belgesine kaydediliyor...\")\n",
    "doc = Document()\n",
    "doc.add_heading('Transkripsiyon Sonuçları', level=1)\n",
    "\n",
    "for segment in result_segments:\n",
    "    speaker = segment['speaker']\n",
    "    text = segment['text']\n",
    "    doc.add_paragraph(f\"{speaker}: {text}\")\n",
    "\n",
    "# Ses dosyasının adını al ve uzantısını '.docx' olarak değiştir\n",
    "audio_filename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "output_filename = f\"{audio_filename}.docx\"\n",
    "\n",
    "# Word belgesini kaydet\n",
    "output_path = os.path.join(\"data\", output_filename)\n",
    "doc.save(output_path)\n",
    "print(f\"Sonuçlar '{output_path}' dosyasına kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "import torch\n",
    "from docx import Document\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time  # Simülasyon amacıyla eklendi\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env dosyasını yükle\n",
    "load_dotenv()\n",
    "# HF Token'ı ENV'den al\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Toplam adım sayısı\n",
    "total_steps = 10\n",
    "\n",
    "# İlerleme çubuğunu başlat\n",
    "pbar = tqdm(total=total_steps, desc=\"İşlem Adımları\", unit=\"adım\")\n",
    "\n",
    "# 1. Adım: Cihazı ve uygun compute_type'ı belirle\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
    "print(f\"Cihaz: {device}, Compute Type: {compute_type}\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 2. Adım: WhisperX modelini yükle\n",
    "print(\"WhisperX modeli yükleniyor...\")\n",
    "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
    "print(\"WhisperX modeli yüklendi.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 3. Adım: 'data' klasöründeki ses dosyasının yolunu belirtin\n",
    "audio_file = os.path.join(\"data\", \"Yönetici_1 Erdem Erdoğdu2.wav\")\n",
    "print(f\"Ses dosyası: {audio_file}\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 4. Adım: Ses dosyasını transkribe et\n",
    "print(\"Ses dosyası transkribe ediliyor...\")\n",
    "result = model.transcribe(audio_file, language='tr')\n",
    "print(\"Transkripsiyon tamamlandı.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 5. Adım: Zaman hizalama modeli ve metadata yükle\n",
    "print(\"Zaman hizalama modeli yükleniyor...\")\n",
    "model_a, metadata = whisperx.load_align_model(language_code='tr', device=device)\n",
    "print(\"Zaman hizalama modeli yüklendi.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 6. Adım: Zaman hizalaması yap\n",
    "print(\"Zaman hizalaması yapılıyor...\")\n",
    "result_aligned = whisperx.align(result[\"segments\"], model_a, metadata, audio_file, device)\n",
    "print(\"Zaman hizalaması tamamlandı.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 7. Adım: Konuşmacı ayrımı modeli yükleniyor\n",
    "print(\"Konuşmacı ayrımı modeli yükleniyor...\")\n",
    "diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=device)\n",
    "print(\"Konuşmacı ayrımı modeli yüklendi.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 8. Adım: Konuşmacı ayrımı yapılıyor\n",
    "print(\"Konuşmacı ayrımı yapılıyor...\")\n",
    "diarize_segments = diarize_model(audio_file)\n",
    "print(\"Konuşmacı ayrımı tamamlandı.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 9. Adım: Konuşmacı bilgileri transkripsiyonla birleştiriliyor\n",
    "print(\"Konuşmacı bilgileri transkripsiyonla birleştiriliyor...\")\n",
    "result_segments, word_segments = whisperx.assign_word_speakers(diarize_segments, result_aligned)\n",
    "print(\"Birleştirme tamamlandı.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 10. Adım: Sonuçlar Word belgesine kaydediliyor\n",
    "\"\"\"\n",
    "print(\"Sonuçlar Word belgesine kaydediliyor...\")\n",
    "doc = Document()\n",
    "doc.add_heading('Transkripsiyon Sonuçları', level=1)\n",
    "\n",
    "for segment in result_segments:\n",
    "    speaker = segment['speaker']\n",
    "    text = segment['text']\n",
    "    doc.add_paragraph(f\"{speaker}: {text}\")\n",
    "\n",
    "# Ses dosyasının adını al ve uzantısını '.docx' olarak değiştir\n",
    "audio_filename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "output_filename = f\"{audio_filename}.docx\"\n",
    "\n",
    "# Word belgesini kaydet\n",
    "output_path = os.path.join(\"data\", output_filename)\n",
    "doc.save(output_path)\n",
    "print(f\"Sonuçlar '{output_path}' dosyasına kaydedildi.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# İlerleme çubuğunu tamamla\n",
    "pbar.close()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "# Word belgesi oluştur\n",
    "doc = Document()\n",
    "\n",
    "# Birleştirilmiş metinleri tutacak değişkenler\n",
    "merged_transcriptions = []\n",
    "current_speaker = None\n",
    "current_text = \"\"\n",
    "\n",
    "# Tüm segmentlerde dolaş\n",
    "for segment in aligned_segments:\n",
    "    speaker = segment.get('speaker', 'Bilinmeyen Konuşmacı')\n",
    "    text = segment['text']\n",
    "\n",
    "    # Eğer konuşmacı değişirse veya ilk segment ise\n",
    "    if current_speaker != speaker:\n",
    "        if current_speaker is not None:  # Önceki konuşmacının metnini kaydet\n",
    "            merged_transcriptions.append(f\"{current_speaker}: {current_text}\")\n",
    "        # Yeni konuşmacı için değişkenleri sıfırla\n",
    "        current_speaker = speaker\n",
    "        current_text = text\n",
    "    else:\n",
    "        # Aynı konuşmacı ise metni birleştir\n",
    "        current_text += f\" {text}\"\n",
    "\n",
    "# Son konuşmacının metnini ekle\n",
    "if current_speaker is not None:\n",
    "    merged_transcriptions.append(f\"{current_speaker}: \\n{current_text}\")\n",
    "\n",
    "# Word belgesine yaz\n",
    "for transcription in merged_transcriptions:\n",
    "    doc.add_paragraph(transcription)\n",
    "\n",
    "# Belgeyi kaydet\n",
    "doc.save(\"birleştirilmiş_transkripsiyon.docx\")\n",
    "print(\"Birleştirilmiş transkripsiyon kaydedildi.\")\n",
    "print(merged_transcriptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Step:   0%|          | 0/10 [00:00<?, ?step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu, Compute Type: int8\n",
      "Loading WhisperX model...\n",
      "No language specified, language will be first be detected for each audio file (increases inference time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\inspect.py:869: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\musta\\Documents\\Python Projects\\tangible\\venv_whisperx\\lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n",
      "Process Step:  20%|██        | 2/10 [00:34<02:18, 17.29s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cpu. Bad things might happen unless you revert torch to 1.x.\n",
      "WhisperX model loaded.\n",
      "Audio file: data\\Görüşme10 Selma Zengin.m4a\n",
      "Transcribing audio file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Step:  40%|████      | 4/10 [1:49:59<3:13:47, 1937.93s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription completed.\n",
      "Loading time alignment model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Step:  50%|█████     | 5/10 [1:50:01<1:53:46, 1365.26s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time alignment model loaded.\n",
      "Processing time alignment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Step:  60%|██████    | 6/10 [2:11:56<1:30:01, 1350.42s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time alignment completed.\n",
      "Loading speaker diarization model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Step:  70%|███████   | 7/10 [2:11:58<47:25, 948.63s/step]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker diarization model loaded.\n",
      "Processing speaker diarization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\Documents\\Python Projects\\tangible\\venv_whisperx\\lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1823.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "Process Step:  80%|████████  | 8/10 [3:29:08<1:08:15, 2047.69s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker diarization completed.\n",
      "Speaker information is being merged with the transcription...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Step:  90%|█████████ | 9/10 [3:29:09<23:55, 1435.59s/step]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed.\n",
      "Saving results to the Word document...\n",
      "Results saved to 'data\\Görüşme10 Selma Zengin.docx'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Step: 100%|██████████| 10/10 [3:29:09<00:00, 1254.93s/step]\n"
     ]
    }
   ],
   "source": [
    "# WhisperX \n",
    "import whisperx\n",
    "import torch\n",
    "from docx import Document\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env dosyasını yükle\n",
    "\n",
    "load_dotenv()\n",
    "# HF Token'ı ENV'den al\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Toplam adım sayısı\n",
    "total_steps = 10\n",
    "\n",
    "# İlerleme çubuğunu başlat\n",
    "pbar = tqdm(total=total_steps, desc=\"Process Step\", unit=\"step\")\n",
    "\n",
    "# 1. Cihazı ve uygun compute_type'ı belirle\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"int8\"\n",
    "print(f\"Device: {device}, Compute Type: {compute_type}\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 2. WhisperX modelini yükle\n",
    "print(\"Loading WhisperX model...\")\n",
    "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
    "print(\"WhisperX model loaded.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 3. 'data' klasöründeki ses dosyasının yolunu belirtin\n",
    "audio_file = os.path.join(\"data\", \"Görüşme10 Selma Zengin.m4a\")\n",
    "print(f\"Audio file: {audio_file}\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 4. Ses dosyasını transkribe et\n",
    "print(\"Transcribing audio file...\")\n",
    "result = model.transcribe(audio_file, language='tr')\n",
    "print(\"Transcription completed.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 5. Zaman hizalama modeli ve metadata yükle\n",
    "print(\"Loading time alignment model...\")\n",
    "model_a, metadata = whisperx.load_align_model(language_code='tr', device=device)\n",
    "print(\"Time alignment model loaded.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 6. Zaman hizalaması yap\n",
    "print(\"Processing time alignment...\")\n",
    "result_aligned = whisperx.align(result[\"segments\"], model_a, metadata, audio_file, device)\n",
    "print(\"Time alignment completed.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 7. Konuşmacı ayrımı modeli yükleniyor\n",
    "print(\"Loading speaker diarization model...\")\n",
    "diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=device)\n",
    "print(\"Speaker diarization model loaded.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 8. Konuşmacı ayrımı yapılıyor\n",
    "print(\"Processing speaker diarization...\")\n",
    "diarize_segments = diarize_model(audio_file)\n",
    "print(\"Speaker diarization completed.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 9. Konuşmacı bilgileri transkripsiyonla birleştiriliyor\n",
    "print(\"Speaker information is being merged with the transcription...\")\n",
    "#result_segments, word_segments = whisperx.assign_word_speakers(diarize_segments, result_aligned)\n",
    "\n",
    "# `result_aligned['segments']` ve `diarize_segments` manuel olarak birleştiriliyor\n",
    "aligned_segments = result_aligned['segments']  # Metin segmentleri\n",
    "diarized_segments = []  # Birleştirilmiş sonuçları saklamak için liste\n",
    "\n",
    "for segment in aligned_segments:\n",
    "    segment_start = segment['start']\n",
    "    segment_end = segment['end']\n",
    "    text = segment['text']\n",
    "\n",
    "    # Konuşmacıyı zaman aralığına göre eşleştir\n",
    "    speaker = \"Unknown Speaker\"  # Varsayılan\n",
    "    for diarize in diarize_segments.itertuples():\n",
    "        if diarize.start <= segment_start <= diarize.end or diarize.start <= segment_end <= diarize.end:\n",
    "            speaker = diarize.speaker\n",
    "            break\n",
    "\n",
    "    # Birleştirilmiş sonucu ekle\n",
    "    diarized_segments.append({'speaker': speaker, 'text': text})\n",
    "\n",
    "# Birleştirilmiş sonuçları işlemek için değişkenler\n",
    "merged_transcriptions = []\n",
    "current_speaker = None\n",
    "current_text = \"\"\n",
    "\n",
    "# Birleştirilmiş segmentlerde dolaş ve konuşmacı metinlerini birleştir\n",
    "for segment in diarized_segments:\n",
    "    speaker = segment['speaker']\n",
    "    text = segment['text']\n",
    "\n",
    "    # Eğer konuşmacı değişirse veya ilk segment ise\n",
    "    if current_speaker != speaker:\n",
    "        if current_speaker is not None:  # Önceki konuşmacının metnini kaydet\n",
    "            merged_transcriptions.append((current_speaker, current_text))\n",
    "        current_speaker = speaker  # Yeni konuşmacıya geç\n",
    "        current_text = text  # Yeni metni başlat\n",
    "    else:\n",
    "        # Aynı konuşmacı ise metni birleştir\n",
    "        current_text += f\" {text}\"\n",
    "\n",
    "# Son konuşmacının metnini ekle\n",
    "if current_speaker is not None:\n",
    "    merged_transcriptions.append((current_speaker, current_text))\n",
    "\n",
    "print(\"Merging completed.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# 10. Adım: Sonuçlar Word belgesine kaydediliyor\n",
    "print(\"Saving results to the Word document...\")\n",
    "audio_filename = os.path.splitext(os.path.basename(audio_file))[0]\n",
    "doc = Document()\n",
    "doc.add_heading(f'Transkripsiyon Sonuçları: {audio_filename}', level=1)\n",
    "\n",
    "# Her konuşmacıyı ve metni ayrı paragraflara yaz\n",
    "for speaker, text in merged_transcriptions:\n",
    "    doc.add_paragraph(f\"{speaker}:\")\n",
    "    doc.add_paragraph(text)\n",
    "\n",
    "# Word dosyasını kaydet\n",
    "output_filename = f\"{audio_filename}.docx\"\n",
    "output_path = os.path.join(\"data\", output_filename)\n",
    "doc.save(output_path)\n",
    "print(f\"Results saved to '{output_path}'.\")\n",
    "pbar.update(1)\n",
    "\n",
    "# İlerleme çubuğunu tamamla\n",
    "pbar.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
