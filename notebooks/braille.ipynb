{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Documents\\Python Projects\\tangible\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Set the path as tangible\n",
    "os.chdir('..')\n",
    "\n",
    "# Check current path \n",
    "print(os.getcwd())\n",
    "# Visualization\n",
    "from src.visualization import plot_categorical_distribution, plot_numerical_distribution, plot_crosstab, plot_scatter, plot_frequency_bar, plot_frequency_heatmap, create_frequency_bar_tangible, create_table, create_striped_table, plot_box_high_contrast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: turkce_6\n"
     ]
    }
   ],
   "source": [
    "# Read the text file\n",
    "file_path = 'data/contractions/turkce_6.txt'\n",
    "base_name = os.path.basename(file_path)\n",
    "document_name = os.path.splitext(base_name)[0]\n",
    "print(\"Document: \" + document_name)\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metin Temizleme ve Tokenizasyon\n",
    "# 1) Küçük harfe dönüştür\n",
    "text = text.lower()\n",
    "\n",
    "# 2) Noktalama işaretlerini sil (regex ile)\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# 3) Sayı vb. temizle\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Tokenization\n",
    "tokens = text.split()  # Boşluk karakterine göre ayırır\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word\n",
      "896      glâsnost\n",
      "29359   glâsyolog\n",
      "36773  glâsyoloji\n",
      "2832       glâyöl\n",
      "                  word\n",
      "34179         başpapaz\n",
      "36866            böyle\n",
      "42939   dinamit lokumu\n",
      "37255           dördüz\n",
      "393           dışlanma\n",
      "...                ...\n",
      "4435         şırıldama\n",
      "16009          şırıltı\n",
      "23427          şırınga\n",
      "25877           şıvgın\n",
      "6973        şığa göçüm\n",
      "\n",
      "[42992 rows x 1 columns]\n",
      "        word  count\n",
      "119      bir    302\n",
      "780       ve    184\n",
      "133       bu    138\n",
      "171       de     95\n",
      "548       ne     68\n",
      "..       ...    ...\n",
      "897       ün      1\n",
      "899    ünsüz      1\n",
      "895  öğretim      1\n",
      "17     allah      1\n",
      "20    alıntı      1\n",
      "\n",
      "[932 rows x 2 columns]\n",
      "     word  count\n",
      "159  daha     21\n"
     ]
    }
   ],
   "source": [
    "# Türkçe sözlük dosyası kullanarak metin temizliği yap (tr_isim_sifat.csv)\n",
    "# Türkçe sözlüğü yükle\n",
    "turkish_dict_path = 'data/contractions/tr_isim_sifat.csv'\n",
    "df_turkish_dict = pd.read_csv(turkish_dict_path)\n",
    "\n",
    "# 1) form2 sütunundaki boş hücreleri \"\" ile doldur\n",
    "df_turkish_dict['form2'] = df_turkish_dict['form2'].fillna(\"\")\n",
    "\n",
    "# 2) Sadece küçük harfe dönüştürerek kelimeleri kümeye ekle\n",
    "turkish_words = set()\n",
    "\"\"\"\n",
    "for col in ['form1', 'form2']:\n",
    "    # Küçük harfe dönüştür ve kümeye ekle\n",
    "    turkish_words.update(df_turkish_dict[col].str.lower().tolist())\n",
    "\"\"\"\n",
    "for col in ['form1', 'form2']:\n",
    "    # Küçük harfe dönüştür, regex ile özel karakterleri temizle ve kümeye ekle\n",
    "    turkish_words.update(\n",
    "        df_turkish_dict[col]\n",
    "        .str.lower()  # Küçük harfe dönüştür\n",
    "        .str.replace(r'[^a-zâçğıöşü\\s]', '', regex=True)  # Sadece harf ve boşlukları bırak\n",
    "        .dropna()  # Boş değerleri çıkar\n",
    "        .tolist()  # Listeye dönüştür\n",
    "    )\n",
    "\n",
    "# Boş değerleri kümeden çıkar (eğer varsa)\n",
    "turkish_words.discard(\"\")\n",
    "df_turkish_words = pd.DataFrame(list(turkish_words), columns=['word'])\n",
    "df_turkish_words = df_turkish_words.sort_values('word', ascending=True)\n",
    "print(df_turkish_words[df_turkish_words['word'].str.startswith('glâ')])\n",
    "print(df_turkish_words)\n",
    "# İlk 10 kelimeyi yazdır\n",
    "\n",
    "#print(\"Türkçe sözlükteki ilk 10 kelime:\", list(turkish_words)[:10])\n",
    "#print(df_turkish_words)\n",
    "\n",
    "# -- Metni temizleme ve tokenize etme --\n",
    "text_path = 'data/contractions/sayi_11.txt'\n",
    "with open(text_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# 1) Küçük harfe dönüştür\n",
    "text = text.lower()\n",
    "\n",
    "# 2) Noktalama işaretlerini sil\n",
    "text = re.sub(r'[^\\w\\sâçğıöşü]', '', text)\n",
    "\n",
    "# 3) Sayıları temizle\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "# 4) Tokenizasyon\n",
    "tokens = text.split()  # Boşluklara göre sözcükleri ayır\n",
    "\n",
    "# 5) Türkçe sözlükte olmayan sözcükleri çıkar\n",
    "filtered_tokens = [word for word in tokens if word in turkish_words]\n",
    "\n",
    "# Sonuçları birleştir\n",
    "#cleaned_text = ' '.join(filtered_tokens)\n",
    "df_cleaned_text = pd.DataFrame(list(filtered_tokens), columns=['word'])\n",
    "df_cleaned_text['count'] = 1\n",
    "df_cleaned_text_word_counts = df_cleaned_text.groupby('word')['count'].sum().reset_index()\n",
    "df_cleaned_text_word_counts = df_cleaned_text_word_counts.sort_values('count', ascending=False)\n",
    "\n",
    "# Temizlenmiş metni yazdır\n",
    "#print(\"Temizlenmiş metin:\", cleaned_text)\n",
    "print(df_cleaned_text_word_counts)\n",
    "print(df_cleaned_text_word_counts[df_cleaned_text_word_counts['word'].str.startswith('daha')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame and Save to CSV file\n",
    "df = pd.DataFrame(tokens, columns=['word'])\n",
    "#df = pd.DataFrame(filtered_tokens, columns=['word'])\n",
    "df['count'] = 1\n",
    "\n",
    "# Gruplayarak frekans (count) değerini hesapla\n",
    "word_counts = df.groupby('word')['count'].sum().reset_index()\n",
    "\n",
    "# Dosya ismini ekle\n",
    "#word_counts['source'] = document_name\n",
    "\n",
    "# Frekansına göre büyükten küçüğe sırala\n",
    "\n",
    "word_counts = word_counts.sort_values('count', ascending=False)\n",
    "\n",
    "# İlk 20 kelimeyi görüntüleyelim\n",
    "print(word_counts)\n",
    "print(word_counts[word_counts['word'].str.startswith('daha')])\n",
    "\n",
    "# CSV dosyasına kaydet\n",
    "\"\"\"csv_path = 'data/contractions/word_frequencies_textbooks.csv'\n",
    "if not os.path.exists(csv_path):\n",
    "    # CSV yoksa oluştur (header dahil)\n",
    "    word_counts.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "else:\n",
    "    # CSV varsa oku ve birleştir (append)\n",
    "    existing_df = pd.read_csv(csv_path)\n",
    "    combined_df = pd.concat([existing_df, word_counts], ignore_index=True)\n",
    "    combined_df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word  count\n",
      "1857          bir   1091\n",
      "11748          ve    929\n",
      "2093           bu    516\n",
      "1095    aşağıdaki    318\n",
      "6310         için    240\n",
      "...           ...    ...\n",
      "14171    şimdilik      1\n",
      "14172     şimdiye      1\n",
      "4519    gelmiştim      1\n",
      "14174   şimşekler      1\n",
      "14175  şimşekleri      1\n",
      "\n",
      "[14208 rows x 2 columns]\n",
      "        word  count\n",
      "2571    daha    127\n",
      "2572  dahası      1\n"
     ]
    }
   ],
   "source": [
    "# Hunspell Türkçe sözlüğü yükle (CSV dosyasından)\n",
    "turkish_dict_path = 'data/contractions/hunspell_tr.csv'\n",
    "df_turkish_dict = pd.read_csv(turkish_dict_path, header=None, names=[\"word\"])\n",
    "\n",
    "# 1) Boş değerleri çıkar ve küçük harfe dönüştür\n",
    "df_turkish_dict['word'] = df_turkish_dict['word'].str.strip().str.lower()\n",
    "df_turkish_dict = df_turkish_dict.dropna()  # Boş satırları sil\n",
    "\n",
    "# 2) Kelimeleri kümeye ekle\n",
    "turkish_words = set(df_turkish_dict['word'].tolist())\n",
    "\n",
    "# Boş değerleri kümeden çıkar (eğer varsa)\n",
    "turkish_words.discard(\"\")  # Herhangi bir boş kelime varsa çıkar\n",
    "\n",
    "# 3) Sözlüğü DataFrame olarak sıralayıp inceleyin\n",
    "df_turkish_words = pd.DataFrame(list(turkish_words), columns=['word'])\n",
    "df_turkish_words = df_turkish_words.sort_values('word', ascending=True)\n",
    "\n",
    "# Sözlükte \"kâ\" ile başlayan kelimeleri yazdır\n",
    "#print(df_turkish_words[df_turkish_words['word'].str.startswith('kâ')])\n",
    "\n",
    "# Tüm sözlüğü yazdır\n",
    "#print(df_turkish_words)\n",
    "\n",
    "# -- Metni temizleme ve tokenize etme --\n",
    "text_path = 'data/contractions/turkce_6.txt'\n",
    "with open(text_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# 1) Küçük harfe dönüştür\n",
    "text = text.lower()\n",
    "\n",
    "# 2) Noktalama işaretlerini sil\n",
    "text = re.sub(r'[^\\w\\sçğıöşü]', '', text)\n",
    "\n",
    "# 3) Sayıları temizle\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "# 4) Tokenizasyon\n",
    "tokens = text.split()  # Boşluklara göre sözcükleri ayır\n",
    "\n",
    "# 5) Sözlükteki kelimelerle başlayan sözcükleri kontrol et\n",
    "def is_valid_word(word, dictionary):\n",
    "    # Kelimenin sözlükteki herhangi bir kelimeyle başlayıp başlamadığını kontrol et\n",
    "    for valid_word in dictionary:\n",
    "        if word.startswith(valid_word):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "filtered_tokens = [word for word in tokens if is_valid_word(word, turkish_words)]\n",
    "\n",
    "# Sonuçları birleştir\n",
    "df_text = pd.DataFrame(list(filtered_tokens), columns=['word'])\n",
    "df_text['count'] = 1\n",
    "df_text_word_counts = df_text.groupby('word')['count'].sum().reset_index()\n",
    "df_text_word_counts = df_text_word_counts.sort_values('count', ascending=False)\n",
    "\n",
    "# Temizlenmiş metni yazdır\n",
    "print(df_text_word_counts)\n",
    "print(df_text_word_counts[df_text_word_counts['word'].str.startswith('daha')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV dosyasına kaydet\n",
    "csv_path = 'data/contractions/word_frequencies_textbooks.csv'\n",
    "if not os.path.exists(csv_path):\n",
    "    # CSV yoksa oluştur (header dahil)\n",
    "    df_text_word_counts.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"CSV dosyası oluşturuldu: {csv_path}\")\n",
    "else:\n",
    "    # CSV dosyası varsa birleştir (append)\n",
    "    existing_df = pd.read_csv(csv_path)\n",
    "    combined_df = pd.concat([existing_df, df_text_word_counts], ignore_index=True)\n",
    "    combined_df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"CSV dosyasına eklendi: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tekrar eden sözcükleri birleştir (Word Frequencies CSV dosyası için)\n",
    "csv_path = 'data/contractions/word_frequencies.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Sözcük bazında topla\n",
    "df_merged = df.groupby('word', as_index=False)['count'].sum()\n",
    "df_merged = df_merged.sort_values('count', ascending=False)\n",
    "\n",
    "# Eğer 'source' (belge adı) sütunu da varsa ve hem 'word' hem de 'source' bazında birleştirmek istiyorsanız:\n",
    "# df_merged = df.groupby(['word', 'source'], as_index=False)['count'].sum()\n",
    "# Ortalamayı ve standart sapmayı hesapla\n",
    "mean_val = df_merged['count'].mean()\n",
    "std_val = df_merged['count'].std()\n",
    "\n",
    "# Standart sapmanın 0 olması ihtimaline karşı kontrol et\n",
    "if std_val == 0:\n",
    "    # Böyle bir durumda tüm z-skorları 0 yapılabilir\n",
    "    df_merged['z_score'] = 0\n",
    "else:\n",
    "    # (count - ortalama) / std formülüyle z-skorunu ekle\n",
    "    df_merged['z_score'] = (df_merged['count'] - mean_val) / std_val\n",
    "\n",
    "# Sonucu incele\n",
    "print(mean_val)\n",
    "print(std_val)\n",
    "print(df_merged)\n",
    "\n",
    "# CSV olarak kaydet\n",
    "#df_merged.to_csv('data/contractions/word_frequencies_merged.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "import nltk\n",
    "\n",
    "# Türkçe stopword'leri indirelim (bir kereye mahsus)\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# NLTK'nin Türkçe stopword'lerini alalım (tamamen yeterli olmayabilir, isteğe göre genişletilebilir)\n",
    "#turkish_stopwords_nltk = stopwords.words('turkish')\n",
    "turkish_stopwords_nltk = []\n",
    "print(turkish_stopwords_nltk)\n",
    "# Ek olarak kendi özel stopword listenizi de tanımlayabilirsiniz\n",
    "csv_path_stopwords = 'data/contractions/stopwords.csv'\n",
    "df_my_stopwords = pd.read_csv(csv_path_stopwords, header=None)\n",
    "my_stopwords = df_my_stopwords[0].tolist()\n",
    "# İki listeyi birleştirelim\n",
    "\n",
    "turkish_stopwords = set(turkish_stopwords_nltk + my_stopwords)\n",
    "\n",
    "# --- Asıl analiz ---\n",
    "\n",
    "# 1) CSV'yi okuyalım\n",
    "csv_path = 'data/contractions/word_frequencies.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 2) Sözcük bazında topla (aynı 'word' birden çok satırda olabilir diye)\n",
    "df_merged = df.groupby('word', as_index=False)['count'].sum()\n",
    "\n",
    "# 3) Stopword’leri çıkaralım\n",
    "df_stopwords_removed = df_merged[~df_merged['word'].isin(turkish_stopwords)]\n",
    "\n",
    "# 4) Temizlenmiş veriyi frekansa göre sırala\n",
    "df_stopwords_removed = df_stopwords_removed.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "#df_stopwords_removed = df_stopwords_removed.sort_values('word', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# 5) Ortalamayı ve standart sapmayı hesaplayalım\n",
    "mean_val_sw = df_stopwords_removed['count'].mean()\n",
    "std_val_sw = df_stopwords_removed['count'].std()\n",
    "\n",
    "# 6) Z-skorunu hesapla\n",
    "if std_val_sw == 0:\n",
    "    df_stopwords_removed['z_score'] = 0\n",
    "else:\n",
    "    df_stopwords_removed['z_score'] = (df_stopwords_removed['count'] - mean_val_sw) / std_val_sw\n",
    "\n",
    "# 7) Sonucu görelim\n",
    "print(mean_val_sw)\n",
    "print(std_val_sw)\n",
    "print(df_stopwords_removed.head(20))\n",
    "\n",
    "# 8) İsterseniz CSV olarak kaydedebilirsiniz\n",
    "# df_stopwords_removed.to_csv('data/contractions/word_frequencies_merged_no_stopwords.csv', \n",
    "#                             index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bir Harfli Kısaltma sayısı: 28\n",
      "Toplam sözcük (Tüm kaynaklar): 280161\n",
      "Toplam frekans (BHK): 20478\n",
      "Yüzde (%): 7.309368541659975 \n",
      "\n",
      "Ortalama: 731.3571428571429\n",
      "Standart sapma: 584.2480583200887 \n",
      "\n",
      "Skewness (Raw Data): 0.5732847221094318\n",
      "Skewness (Log-Transfer): -1.6880981389905834\n",
      "Skewness (Box-Cox): -0.23893705905335075 \n",
      "\n",
      "  word  count  percentage  z_score_boxcox   z_score z_categories\n",
      "0  çok   2001    9.771462        1.643426  2.173123   çok yüksek\n",
      "       word  count  percentage  z_score_boxcox   z_score z_categories\n",
      "26     eğer      5    0.024416       -1.927272 -1.243234    çok düşük\n",
      "27  ekonomi      5    0.024416       -1.927272 -1.243234    çok düşük\n"
     ]
    }
   ],
   "source": [
    "# Bir Harfli Kısaltmalar: Z-Skoru\n",
    "\n",
    "# CSV'yi oku\n",
    "csv_path = 'data/contractions/word_frequencies_novels.csv'\n",
    "df_single = pd.read_csv(csv_path)\n",
    "\n",
    "# Tek harfli kısaltmalar listesi\n",
    "csv_path_con_single = 'data/contractions/con_single.csv'\n",
    "df_con_single = pd.read_csv(csv_path_con_single, header=None)\n",
    "contractions_list_single = df_con_single[0].tolist()\n",
    "print(f\"Bir Harfli Kısaltma sayısı: {df_con_single.size}\")\n",
    "\n",
    "# Stopwordleri çıkar\n",
    "csv_path_stopwords_single = 'data/contractions/stopwords_single.csv'\n",
    "df_my_stopwords_single = pd.read_csv(csv_path_stopwords_single, header=None)\n",
    "my_stopwords_single = df_my_stopwords_single[0].tolist()\n",
    "df_single = df_single[~df_single['word'].isin(my_stopwords_single)]\n",
    "print(f\"Toplam sözcük (Tüm kaynaklar): {df_single['count'].sum()}\")\n",
    "\n",
    "# 1) Listedeki sözcüklerle BAŞLAYAN tüm sözcüklerin toplam frekansını hesapla\n",
    "results = []\n",
    "for cword in contractions_list_single:\n",
    "    sub_count = df_single[df_single['word'].str.startswith(cword, na=False)]['count'].sum()\n",
    "\n",
    "    # Ünsüz yumuşamasına uğramış sözcükler varsa\n",
    "    if cword == \"büyük\":\n",
    "        sub_count += df_single[df_single['word'].str.startswith(\"büyüğ\", na=False)]['count'].sum()\n",
    "    elif cword == \"artık\":\n",
    "        sub_count += df_single[df_single['word'].str.startswith(\"artığ\", na=False)]['count'].sum()\n",
    "\n",
    "    results.append({'word': cword, 'count': sub_count})\n",
    "\n",
    "df_single_freq = pd.DataFrame(results)\n",
    "print(f\"Toplam frekans (BHK): {df_single_freq['count'].sum()}\")\n",
    "print(f\"Yüzde (%): {df_single_freq['count'].sum() / df_single['count'].sum() * 100} \\n\")\n",
    "\n",
    "\n",
    "# 2) Ortalamayı ve standart sapmayı hesapla\n",
    "mean_val_single = df_single_freq['count'].mean()\n",
    "std_val_single = df_single_freq['count'].std()\n",
    "\n",
    "# 3) Standart sapmanın 0 olması ihtimaline karşı kontrol yap\n",
    "if std_val_single == 0:\n",
    "    # z-skorlarını 0 yap\n",
    "    df_single_freq['z_score'] = 0\n",
    "else:\n",
    "    # (count - ortalama) / std formülüyle z-skorunu ekle\n",
    "    df_single_freq['z_score'] = (df_single_freq['count'] - mean_val_single) / std_val_single\n",
    "\n",
    "# 4) Dönüşüm Uygula\n",
    "# Log-Transfer uygula\n",
    "# 'count' değerleri için log(1+x) dönüşümü\n",
    "df_single_freq['log_count'] = np.log1p(df_single_freq['count'])  # log(count + 1)\n",
    "# Box-Cox Dönüşümü\n",
    "# df_single_freq['count'] içinde sıfır veya çok küçük değerler varsa:\n",
    "df_single_freq['count_shifted'] = df_single_freq['count'] + 1\n",
    "# boxcox fonksiyonu iki değer döndürür: \n",
    "# 1. dönüşmüş veriler (transformed), 2. bulduğu en uygun lambda değeri (fitted_lambda)\n",
    "transformed_values, fitted_lambda = boxcox(df_single_freq['count_shifted'])\n",
    "df_single_freq['boxcox_count'] = transformed_values\n",
    "\n",
    "#print(\"Box-Cox için bulunan en uygun lambda değeri:\", fitted_lambda)\n",
    "\n",
    "# 5) Dönüşüm Sonrası Ortalama ve Standart Sapma Hesapla\n",
    "# Log değerlerin ortalama ve STD'sini al\n",
    "mean_log = df_single_freq['log_count'].mean()\n",
    "std_log = df_single_freq['log_count'].std()\n",
    "# Box-Cox değerlerinin ortalama ve STD'sini al\n",
    "mean_boxcox = df_single_freq['boxcox_count'].mean()\n",
    "std_boxcox = df_single_freq['boxcox_count'].std()\n",
    "\n",
    "# 6) Dönüşüm Sonrası Z-Skoru Hesapla\n",
    "# Log değer üzerinden z-skoru\n",
    "if std_log == 0:\n",
    "    df_single_freq['z_score_log'] = 0\n",
    "else:\n",
    "    df_single_freq['z_score_log'] = (df_single_freq['log_count'] - mean_log) / std_log\n",
    "# Box-Cox değer üzerinden z-skoru\n",
    "if std_boxcox == 0:\n",
    "    df_single_freq['z_score_boxcox'] = 0\n",
    "else:\n",
    "    df_single_freq['z_score_boxcox'] = (df_single_freq['boxcox_count'] - mean_boxcox) / std_boxcox\n",
    "\n",
    "# 7) Z-Skoruna göre kategorilere ayır\n",
    "# Kategoriler için bin aralıkları\n",
    "bins = [-np.inf, -1.5, -0.5, 0.5, 1.5, np.inf]\n",
    "labels = [\"çok düşük\", \"düşük\", \"orta\", \"yüksek\", \"çok yüksek\"]\n",
    "df_single_freq['z_categories'] = pd.cut(df_single_freq['z_score_boxcox'], bins=bins, labels=labels)\n",
    "\n",
    "# 8) Yüzdelik hesapla\n",
    "# Toplam 'count' değerini hesapla\n",
    "total_count_single = df_single_freq['count'].sum()\n",
    "# Yeni sütun: 'percentage' (yüzde)\n",
    "df_single_freq['percentage'] = (df_single_freq['count'] / total_count_single) * 100\n",
    "df_single_percent = df_single_freq[['word', 'count', 'percentage', 'z_score_boxcox', 'z_score', 'z_categories']]\n",
    "\n",
    "# 9) Sırala ve sonucu yazdır\n",
    "df_single_freq = df_single_freq.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "df_single_percent = df_single_percent.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "print(f\"Ortalama: {mean_val_single}\")\n",
    "print(f\"Standart sapma: {std_val_single} \\n\")\n",
    "print(f\"Skewness (Raw Data): {df_single_percent['count'].skew()}\")\n",
    "print(f\"Skewness (Log-Transfer): {df_single_freq['log_count'].skew()}\")\n",
    "print(f\"Skewness (Box-Cox): {df_single_freq['boxcox_count'].skew()} \\n\")\n",
    "# Histogram ve Boxplot\n",
    "#df_single_percent['count'].hist()\n",
    "#plt.show()\n",
    "#plot_box_high_contrast(df_single_percent)\n",
    "#plot_box_high_contrast(df_single_freq, column='log_count')\n",
    "#plot_box_high_contrast(df_single_freq, column='boxcox_count')\n",
    "# Filtreleme yap\n",
    "\n",
    "print(df_single_percent[df_single_percent['z_categories'] == 'çok yüksek'])\n",
    "print(df_single_percent[df_single_percent['z_categories'] == 'çok düşük'])\n",
    "#print(df_single_percent[df_single_percent['word'] == 'büyük'])\n",
    "#TODO: Yeo-Johnson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/contractions/word_frequencies.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# İki Harfli Kısaltmalar: Z-Skoru\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# CSV'yi oku\u001b[39;00m\n\u001b[0;32m      4\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/contractions/word_frequencies.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# İki harfli kısaltmalar listesi\u001b[39;00m\n\u001b[0;32m      8\u001b[0m csv_path_con_double \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/contractions/con_double.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\musta\\Documents\\Python Projects\\tangible\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\musta\\Documents\\Python Projects\\tangible\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\musta\\Documents\\Python Projects\\tangible\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\musta\\Documents\\Python Projects\\tangible\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\musta\\Documents\\Python Projects\\tangible\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/contractions/word_frequencies.csv'"
     ]
    }
   ],
   "source": [
    "# İki Harfli Kısaltmalar: Z-Skoru\n",
    "\n",
    "# CSV'yi oku\n",
    "csv_path = 'data/contractions/word_frequencies.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# İki harfli kısaltmalar listesi\n",
    "csv_path_con_double = 'data/contractions/con_double.csv'\n",
    "df_con_double = pd.read_csv(csv_path_con_double, header=None)\n",
    "# 2. sütundaki değerleri listeye al\n",
    "contractions_double = df_con_double[1].tolist()\n",
    "print(contractions_double)\n",
    "\n",
    "# 1) Listedeki her sözcükle BAŞLAYAN tüm sözcüklerin toplam frekansını bulalım\n",
    "results = []\n",
    "for cword in contractions_double:\n",
    "    sub_count = df[df['word'].str.startswith(cword, na=False)]['count'].sum()\n",
    "    # Ünsüz yumuşaması varsa frekansa ekle\n",
    "    if cword == \"cevap\":\n",
    "        sub_count += df[df['word'].str.startswith(\"cevab\", na=False)]['count'].sum()\n",
    "    elif cword == \"çeşit\":\n",
    "        sub_count += df[df['word'].str.startswith(\"çeşid\", na=False)]['count'].sum()\n",
    "    elif cword == \"çocuk\":\n",
    "        sub_count += df[df['word'].str.startswith(\"çocuğ\", na=False)]['count'].sum()\n",
    "    elif cword == \"kitap\":\n",
    "        sub_count += df[df['word'].str.startswith(\"kitab\", na=False)]['count'].sum()\n",
    "    elif cword == \"küçük\":\n",
    "        sub_count += df[df['word'].str.startswith(\"küçüğ\", na=False)]['count'].sum()\n",
    "    elif cword == \"sebep\":\n",
    "        sub_count += df[df['word'].str.startswith(\"sebeb\", na=False)]['count'].sum()\n",
    "    elif cword == \"sıcak\":\n",
    "        sub_count += df[df['word'].str.startswith(\"sıcağ\", na=False)]['count'].sum()\n",
    "    elif cword == \"soğuk\":\n",
    "        sub_count += df[df['word'].str.startswith(\"soğuğ\", na=False)]['count'].sum()\n",
    "    elif cword == \"toprak\":\n",
    "        sub_count += df[df['word'].str.startswith(\"toprağ\", na=False)]['count'].sum()\n",
    "    # Ünlü düşmesi varsa frekansa ekle\n",
    "    elif cword == \"cisim\":\n",
    "        sub_count += df[df['word'].str.startswith(\"cism\", na=False)]['count'].sum()\n",
    "    elif cword == \"kısım\":\n",
    "        sub_count += df[df['word'].str.startswith(\"kısm\", na=False)]['count'].sum()\n",
    "\n",
    "    results.append({'word': cword, 'count': sub_count})\n",
    "\n",
    "df_double_freq = pd.DataFrame(results)\n",
    "\n",
    "# 2) Ortalamayı ve standart sapmayı hesapla\n",
    "mean_val_double = df_double_freq['count'].mean()\n",
    "std_val_double = df_double_freq['count'].std()\n",
    "\n",
    "# 3) Standart sapmanın 0 olması ihtimaline karşı kontrol yap\n",
    "if std_val_double == 0:\n",
    "    df_double_freq['z_score'] = 0\n",
    "else:\n",
    "    # (count - ortalama) / std formülüyle z-skorunu ekleyelim\n",
    "\n",
    "    df_double_freq['z_score'] = (df_double_freq['count'] - mean_val_double) / std_val_double\n",
    "\n",
    "# 4) Log-Transfer uygula\n",
    "# 'count' değerleri için log(1+x) dönüşümü\n",
    "df_double_freq['log_count'] = np.log1p(df_double_freq['count'])  # log(count + 1)\n",
    "\n",
    "# 5) Log değerlerin ortalamasını ve std'sini alın\n",
    "mean_double_log = df_double_freq['log_count'].mean()\n",
    "std_double_log = df_double_freq['log_count'].std()\n",
    "\n",
    "# 6) Log değer üzerinden z-skoru\n",
    "if std_double_log == 0:\n",
    "    df_double_freq['z_score_log'] = 0\n",
    "else:\n",
    "    df_double_freq['z_score_log'] = (df_double_freq['log_count'] - mean_double_log) / std_double_log\n",
    "\n",
    "# 7) Z-Skoruna göre kategorilere ayır\n",
    "# Kategoriler için bin aralıkları (soldan sağa sıralı):\n",
    "bins = [-np.inf, -1, -0.5, 0, 0.5, 1, np.inf]\n",
    "# Bu aralıklara denk gelen etiketler (labels):\n",
    "labels = [\"çok düşük\", \"düşük\", \"orta düşük\", \"orta yüksek\", \"yüksek\", \"çok yüksek\"]\n",
    "df_double_freq['z_categories'] = pd.cut(df_double_freq['z_score_log'], bins=bins, labels=labels)\n",
    "\n",
    "# 8) Yüzdelik hesapla\n",
    "# Toplam 'count' değerini hesapla\n",
    "total_count_double = df_double_freq['count'].sum()\n",
    "# Yeni sütun: 'percentage' (yüzde)\n",
    "df_double_freq['percentage'] = (df_double_freq['count'] / total_count_double) * 100\n",
    "df_double_percent = df_double_freq[['word', 'count', 'percentage', 'z_score_log', 'z_categories']]\n",
    "\n",
    "\n",
    "# 9) Sırala ve sonucu yazdır\n",
    "df_double_percent = df_double_percent.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "print(mean_val_double)\n",
    "print(std_val_double)\n",
    "#print(df_double_percent)\n",
    "# Çok düşük olanları sırala\n",
    "print(df_double_percent[df_double_percent['z_categories'] == 'çok yüksek'])\n",
    "#print(df_double_percent[df_double_percent['word'] == 'küçük'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
