{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\Documents\\Python Projects\\tangible\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Set the path as tangible\n",
    "os.chdir('..')\n",
    "\n",
    "# Check current path \n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: sayi_11\n"
     ]
    }
   ],
   "source": [
    "# Read the text file\n",
    "file_path = 'data/contractions/sayi_11.txt'\n",
    "base_name = os.path.basename(file_path)\n",
    "document_name = os.path.splitext(base_name)[0]\n",
    "print(\"Document: \" + document_name)\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metin Temizleme ve Tokenizasyon\n",
    "# 1) Küçük harfe dönüştür\n",
    "text = text.lower()\n",
    "\n",
    "# 2) Noktalama işaretlerini sil (regex ile)\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# 3) Sayı vb. temizle\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Tokenization\n",
    "tokens = text.split()  # Boşluk karakterine göre ayırır\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word\n",
      "896      glâsnost\n",
      "29359   glâsyolog\n",
      "36773  glâsyoloji\n",
      "2832       glâyöl\n",
      "                  word\n",
      "34179         başpapaz\n",
      "36866            böyle\n",
      "42939   dinamit lokumu\n",
      "37255           dördüz\n",
      "393           dışlanma\n",
      "...                ...\n",
      "4435         şırıldama\n",
      "16009          şırıltı\n",
      "23427          şırınga\n",
      "25877           şıvgın\n",
      "6973        şığa göçüm\n",
      "\n",
      "[42992 rows x 1 columns]\n",
      "        word  count\n",
      "119      bir    302\n",
      "780       ve    184\n",
      "133       bu    138\n",
      "171       de     95\n",
      "548       ne     68\n",
      "..       ...    ...\n",
      "897       ün      1\n",
      "899    ünsüz      1\n",
      "895  öğretim      1\n",
      "17     allah      1\n",
      "20    alıntı      1\n",
      "\n",
      "[932 rows x 2 columns]\n",
      "     word  count\n",
      "159  daha     21\n"
     ]
    }
   ],
   "source": [
    "# Türkçe sözlük dosyası kullanarak metin temizliği yap\n",
    "# Türkçe sözlüğü yükle\n",
    "turkish_dict_path = 'data/contractions/tr_isim_sifat.csv'\n",
    "df_turkish_dict = pd.read_csv(turkish_dict_path)\n",
    "\n",
    "# 1) form2 sütunundaki boş hücreleri \"\" ile doldur\n",
    "df_turkish_dict['form2'] = df_turkish_dict['form2'].fillna(\"\")\n",
    "\n",
    "# 2) Sadece küçük harfe dönüştürerek kelimeleri kümeye ekle\n",
    "turkish_words = set()\n",
    "\"\"\"\n",
    "for col in ['form1', 'form2']:\n",
    "    # Küçük harfe dönüştür ve kümeye ekle\n",
    "    turkish_words.update(df_turkish_dict[col].str.lower().tolist())\n",
    "\"\"\"\n",
    "for col in ['form1', 'form2']:\n",
    "    # Küçük harfe dönüştür, regex ile özel karakterleri temizle ve kümeye ekle\n",
    "    turkish_words.update(\n",
    "        df_turkish_dict[col]\n",
    "        .str.lower()  # Küçük harfe dönüştür\n",
    "        .str.replace(r'[^a-zâçğıöşü\\s]', '', regex=True)  # Sadece harf ve boşlukları bırak\n",
    "        .dropna()  # Boş değerleri çıkar\n",
    "        .tolist()  # Listeye dönüştür\n",
    "    )\n",
    "\n",
    "# Boş değerleri kümeden çıkar (eğer varsa)\n",
    "turkish_words.discard(\"\")\n",
    "df_turkish_words = pd.DataFrame(list(turkish_words), columns=['word'])\n",
    "df_turkish_words = df_turkish_words.sort_values('word', ascending=True)\n",
    "print(df_turkish_words[df_turkish_words['word'].str.startswith('glâ')])\n",
    "print(df_turkish_words)\n",
    "# İlk 10 kelimeyi yazdır\n",
    "\n",
    "#print(\"Türkçe sözlükteki ilk 10 kelime:\", list(turkish_words)[:10])\n",
    "#print(df_turkish_words)\n",
    "\n",
    "# -- Metni temizleme ve tokenize etme --\n",
    "text_path = 'data/contractions/sayi_11.txt'\n",
    "with open(text_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# 1) Küçük harfe dönüştür\n",
    "text = text.lower()\n",
    "\n",
    "# 2) Noktalama işaretlerini sil\n",
    "text = re.sub(r'[^\\w\\sâçğıöşü]', '', text)\n",
    "\n",
    "# 3) Sayıları temizle\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "# 4) Tokenizasyon\n",
    "tokens = text.split()  # Boşluklara göre sözcükleri ayır\n",
    "\n",
    "# 5) Türkçe sözlükte olmayan sözcükleri çıkar\n",
    "filtered_tokens = [word for word in tokens if word in turkish_words]\n",
    "\n",
    "# Sonuçları birleştir\n",
    "#cleaned_text = ' '.join(filtered_tokens)\n",
    "df_cleaned_text = pd.DataFrame(list(filtered_tokens), columns=['word'])\n",
    "df_cleaned_text['count'] = 1\n",
    "df_cleaned_text_word_counts = df_cleaned_text.groupby('word')['count'].sum().reset_index()\n",
    "df_cleaned_text_word_counts = df_cleaned_text_word_counts.sort_values('count', ascending=False)\n",
    "\n",
    "# Temizlenmiş metni yazdır\n",
    "#print(\"Temizlenmiş metin:\", cleaned_text)\n",
    "print(df_cleaned_text_word_counts)\n",
    "print(df_cleaned_text_word_counts[df_cleaned_text_word_counts['word'].str.startswith('daha')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         word\n",
      "29794  kâbusu\n",
      "          word\n",
      "51029       ab\n",
      "43440      aba\n",
      "72915     abac\n",
      "20536   abajur\n",
      "3193    abaküs\n",
      "...        ...\n",
      "8869     şırıl\n",
      "25315   şırıld\n",
      "44267  şırıltı\n",
      "65559  şırınga\n",
      "20382   şıvgın\n",
      "\n",
      "[75908 rows x 1 columns]\n",
      "             word  count\n",
      "725           bir    302\n",
      "4686           ve    184\n",
      "813            bu    138\n",
      "1061           de     95\n",
      "1014           da     85\n",
      "...           ...    ...\n",
      "1007   cümlelerin      1\n",
      "1008  cümlelerini      1\n",
      "1009         cıla      1\n",
      "5612    şükrettim      1\n",
      "4912   yaşlarında      1\n",
      "\n",
      "[5614 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Hunspell Türkçe sözlüğü yükle (CSV dosyasından)\n",
    "turkish_dict_path = 'data/contractions/hunspell_tr.csv'\n",
    "df_turkish_dict = pd.read_csv(turkish_dict_path, header=None, names=[\"word\"])\n",
    "\n",
    "# 1) Boş değerleri çıkar ve küçük harfe dönüştür\n",
    "df_turkish_dict['word'] = df_turkish_dict['word'].str.strip().str.lower()\n",
    "df_turkish_dict = df_turkish_dict.dropna()  # Boş satırları sil\n",
    "\n",
    "# 2) Kelimeleri kümeye ekle\n",
    "turkish_words = set(df_turkish_dict['word'].tolist())\n",
    "\n",
    "# Boş değerleri kümeden çıkar (eğer varsa)\n",
    "turkish_words.discard(\"\")  # Herhangi bir boş kelime varsa çıkar\n",
    "\n",
    "# 3) Sözlüğü DataFrame olarak sıralayıp inceleyin\n",
    "df_turkish_words = pd.DataFrame(list(turkish_words), columns=['word'])\n",
    "df_turkish_words = df_turkish_words.sort_values('word', ascending=True)\n",
    "\n",
    "# Sözlükte \"kâ\" ile başlayan kelimeleri yazdır\n",
    "#print(df_turkish_words[df_turkish_words['word'].str.startswith('kâ')])\n",
    "\n",
    "# Tüm sözlüğü yazdır\n",
    "#print(df_turkish_words)\n",
    "\n",
    "# -- Metni temizleme ve tokenize etme --\n",
    "text_path = 'data/contractions/sayi_11.txt'\n",
    "with open(text_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# 1) Küçük harfe dönüştür\n",
    "text = text.lower()\n",
    "\n",
    "# 2) Noktalama işaretlerini sil\n",
    "text = re.sub(r'[^\\w\\sçğıöşü]', '', text)\n",
    "\n",
    "# 3) Sayıları temizle\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "# 4) Tokenizasyon\n",
    "tokens = text.split()  # Boşluklara göre sözcükleri ayır\n",
    "\n",
    "# 5) Sözlükteki kelimelerle başlayan sözcükleri kontrol et\n",
    "def is_valid_word(word, dictionary):\n",
    "    # Kelimenin sözlükteki herhangi bir kelimeyle başlayıp başlamadığını kontrol et\n",
    "    for valid_word in dictionary:\n",
    "        if word.startswith(valid_word):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "filtered_tokens = [word for word in tokens if is_valid_word(word, turkish_words)]\n",
    "\n",
    "# Sonuçları birleştir\n",
    "df_cleaned_text = pd.DataFrame(list(filtered_tokens), columns=['word'])\n",
    "df_cleaned_text['count'] = 1\n",
    "df_cleaned_text_word_counts = df_cleaned_text.groupby('word')['count'].sum().reset_index()\n",
    "df_cleaned_text_word_counts = df_cleaned_text_word_counts.sort_values('count', ascending=False)\n",
    "\n",
    "# Temizlenmiş metni yazdır\n",
    "print(df_cleaned_text_word_counts)\n",
    "print(df_cleaned_text_word_counts[df_cleaned_text_word_counts['word'].str.startswith('daha')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word  count\n",
      "731           bir    302\n",
      "4692           ve    184\n",
      "819            bu    138\n",
      "1067           de     95\n",
      "1020           da     85\n",
      "...           ...    ...\n",
      "1009  cümlelerden      1\n",
      "1010    cümlelere      1\n",
      "1011    cümleleri      1\n",
      "5629    şükrettim      1\n",
      "5615     şimdilik      1\n",
      "\n",
      "[5631 rows x 2 columns]\n",
      "      word  count\n",
      "1021  daha     21\n"
     ]
    }
   ],
   "source": [
    "# Create data frame\n",
    "df = pd.DataFrame(tokens, columns=['word'])\n",
    "#df = pd.DataFrame(filtered_tokens, columns=['word'])\n",
    "df['count'] = 1\n",
    "\n",
    "# Gruplayarak frekans (count) değerini hesapla\n",
    "word_counts = df.groupby('word')['count'].sum().reset_index()\n",
    "\n",
    "# Dosya ismini ekle\n",
    "#word_counts['source'] = document_name\n",
    "\n",
    "# Frekansına göre büyükten küçüğe sırala\n",
    "\n",
    "word_counts = word_counts.sort_values('count', ascending=False)\n",
    "\n",
    "# İlk 20 kelimeyi görüntüleyelim\n",
    "print(word_counts)\n",
    "print(word_counts[word_counts['word'].str.startswith('daha')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV dosyasına kaydet\n",
    "csv_path = 'data/contractions/word_frequencies_oe_cocuk.csv'\n",
    "if not os.path.exists(csv_path):\n",
    "    # CSV yoksa oluştur (header dahil)\n",
    "    word_counts.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "else:\n",
    "    # CSV varsa oku ve birleştir (append)\n",
    "    existing_df = pd.read_csv(csv_path)\n",
    "    combined_df = pd.concat([existing_df, word_counts], ignore_index=True)\n",
    "    combined_df.to_csv(csv_path, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tekrar eden sözcükleri birleştir (Word Frequencies CSV dosyası için)\n",
    "csv_path = 'data/contractions/word_frequencies.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Sözcük bazında topla\n",
    "df_merged = df.groupby('word', as_index=False)['count'].sum()\n",
    "df_merged = df_merged.sort_values('count', ascending=False)\n",
    "\n",
    "# Eğer 'source' (belge adı) sütunu da varsa ve hem 'word' hem de 'source' bazında birleştirmek istiyorsanız:\n",
    "# df_merged = df.groupby(['word', 'source'], as_index=False)['count'].sum()\n",
    "# Ortalamayı ve standart sapmayı hesapla\n",
    "mean_val = df_merged['count'].mean()\n",
    "std_val = df_merged['count'].std()\n",
    "\n",
    "# Standart sapmanın 0 olması ihtimaline karşı kontrol et\n",
    "if std_val == 0:\n",
    "    # Böyle bir durumda tüm z-skorları 0 yapılabilir\n",
    "    df_merged['z_score'] = 0\n",
    "else:\n",
    "    # (count - ortalama) / std formülüyle z-skorunu ekle\n",
    "    df_merged['z_score'] = (df_merged['count'] - mean_val) / std_val\n",
    "\n",
    "# Sonucu incele\n",
    "print(mean_val)\n",
    "print(std_val)\n",
    "print(df_merged)\n",
    "\n",
    "# CSV olarak kaydet\n",
    "#df_merged.to_csv('data/contractions/word_frequencies_merged.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "import nltk\n",
    "\n",
    "# Türkçe stopword'leri indirelim (bir kereye mahsus)\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# NLTK'nin Türkçe stopword'lerini alalım (tamamen yeterli olmayabilir, isteğe göre genişletilebilir)\n",
    "#turkish_stopwords_nltk = stopwords.words('turkish')\n",
    "turkish_stopwords_nltk = []\n",
    "print(turkish_stopwords_nltk)\n",
    "# Ek olarak kendi özel stopword listenizi de tanımlayabilirsiniz\n",
    "csv_path_stopwords = 'data/contractions/stopwords.csv'\n",
    "df_my_stopwords = pd.read_csv(csv_path_stopwords, header=None)\n",
    "my_stopwords = df_my_stopwords[0].tolist()\n",
    "# İki listeyi birleştirelim\n",
    "\n",
    "turkish_stopwords = set(turkish_stopwords_nltk + my_stopwords)\n",
    "\n",
    "# --- Asıl analiz ---\n",
    "\n",
    "# 1) CSV'yi okuyalım\n",
    "csv_path = 'data/contractions/word_frequencies.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 2) Sözcük bazında topla (aynı 'word' birden çok satırda olabilir diye)\n",
    "df_merged = df.groupby('word', as_index=False)['count'].sum()\n",
    "\n",
    "# 3) Stopword’leri çıkaralım\n",
    "df_stopwords_removed = df_merged[~df_merged['word'].isin(turkish_stopwords)]\n",
    "\n",
    "# 4) Temizlenmiş veriyi frekansa göre sırala\n",
    "df_stopwords_removed = df_stopwords_removed.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "#df_stopwords_removed = df_stopwords_removed.sort_values('word', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# 5) Ortalamayı ve standart sapmayı hesaplayalım\n",
    "mean_val_sw = df_stopwords_removed['count'].mean()\n",
    "std_val_sw = df_stopwords_removed['count'].std()\n",
    "\n",
    "# 6) Z-skorunu hesapla\n",
    "if std_val_sw == 0:\n",
    "    df_stopwords_removed['z_score'] = 0\n",
    "else:\n",
    "    df_stopwords_removed['z_score'] = (df_stopwords_removed['count'] - mean_val_sw) / std_val_sw\n",
    "\n",
    "# 7) Sonucu görelim\n",
    "print(mean_val_sw)\n",
    "print(std_val_sw)\n",
    "print(df_stopwords_removed.head(20))\n",
    "\n",
    "# 8) İsterseniz CSV olarak kaydedebilirsiniz\n",
    "# df_stopwords_removed.to_csv('data/contractions/word_frequencies_merged_no_stopwords.csv', \n",
    "#                             index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam frekans: 662\n",
      "Ortalama: 23.642857142857142\n",
      "Standart sapma: 21.279818792242754\n",
      "     word  count  percentage  z_score_log z_categories\n",
      "18   aynı     13    1.963746    -0.038757   orta düşük\n",
      "19   kısa     10    1.510574    -0.234706   orta düşük\n",
      "20   para     10    1.510574    -0.234706   orta düşük\n",
      "21  neden      7    1.057402    -0.493455   orta düşük\n"
     ]
    }
   ],
   "source": [
    "# Bir Harfli Kısaltmalar: Z-Skoru\n",
    "\n",
    "# CSV'yi oku\n",
    "csv_path = 'data/contractions/word_frequencies_oe_cocuk.csv'\n",
    "df_single = pd.read_csv(csv_path)\n",
    "\n",
    "# Tek harfli kısaltmalar listesi\n",
    "csv_path_con_single = 'data/contractions/con_single.csv'\n",
    "df_con_single = pd.read_csv(csv_path_con_single, header=None)\n",
    "contractions_list_single = df_con_single[0].tolist()\n",
    "\n",
    "# Stopwordleri çıkar\n",
    "csv_path_stopwords_single = 'data/contractions/stopwords_single.csv'\n",
    "df_my_stopwords_single = pd.read_csv(csv_path_stopwords_single, header=None)\n",
    "my_stopwords_single = df_my_stopwords_single[0].tolist()\n",
    "df_single = df_single[~df_single['word'].isin(my_stopwords_single)]\n",
    "#print(df_single)\n",
    "\n",
    "# 1) Listedeki sözcüklerle BAŞLAYAN tüm sözcüklerin toplam frekansını hesapla\n",
    "results = []\n",
    "for cword in contractions_list_single:\n",
    "    sub_count = df_single[df_single['word'].str.startswith(cword, na=False)]['count'].sum()\n",
    "\n",
    "    # Ünsüz yumuşamasına uğramış sözcükler varsa\n",
    "    if cword == \"büyük\":\n",
    "        sub_count += df_single[df_single['word'].str.startswith(\"büyüğ\", na=False)]['count'].sum()\n",
    "    elif cword == \"artık\":\n",
    "        sub_count += df_single[df_single['word'].str.startswith(\"artığ\", na=False)]['count'].sum()\n",
    "\n",
    "    results.append({'word': cword, 'count': sub_count})\n",
    "\n",
    "df_single_freq = pd.DataFrame(results)\n",
    "#print(df_single_freq)\n",
    "print(f\"Toplam frekans: {df_single_freq['count'].sum()}\")\n",
    "# 2) Ortalamayı ve standart sapmayı hesapla\n",
    "mean_val_single = df_single_freq['count'].mean()\n",
    "std_val_single = df_single_freq['count'].std()\n",
    "\n",
    "# 3) Standart sapmanın 0 olması ihtimaline karşı kontrol yap\n",
    "if std_val_single == 0:\n",
    "    # Böyle bir durumda tüm z-skorlarını 0 yap\n",
    "    df_single_freq['z_score'] = 0\n",
    "else:\n",
    "    # (count - ortalama) / std formülüyle z-skorunu ekleyelim\n",
    "    df_single_freq['z_score'] = (df_single_freq['count'] - mean_val_single) / std_val_single\n",
    "\n",
    "# 4) Log-Transfer uygula\n",
    "\n",
    "# 'count' değerleri için log(1+x) dönüşümü\n",
    "df_single_freq['log_count'] = np.log1p(df_single_freq['count'])  # log(count + 1)\n",
    "\n",
    "# 5) Log değerlerin ortalamasını ve std'sini alın\n",
    "mean_log = df_single_freq['log_count'].mean()\n",
    "std_log = df_single_freq['log_count'].std()\n",
    "\n",
    "# 6) Log değer üzerinden z-skoru\n",
    "if std_log == 0:\n",
    "    df_single_freq['z_score_log'] = 0\n",
    "else:\n",
    "    df_single_freq['z_score_log'] = (df_single_freq['log_count'] - mean_log) / std_log\n",
    "\n",
    "# 7) Z-Skoruna göre kategorilere ayır\n",
    "# Kategoriler için bin aralıkları (soldan sağa sıralı):\n",
    "bins = [-np.inf, -1, -0.5, 0, 0.5, 1, np.inf]\n",
    "# Bu aralıklara denk gelen etiketler (labels):\n",
    "labels = [\"çok düşük\", \"düşük\", \"orta düşük\", \"orta yüksek\", \"yüksek\", \"çok yüksek\"]\n",
    "df_single_freq['z_categories'] = pd.cut(df_single_freq['z_score_log'], bins=bins, labels=labels)\n",
    "\n",
    "# 8) Yüzdelik hesapla\n",
    "# Toplam 'count' değerini hesapla\n",
    "total_count_single = df_single_freq['count'].sum()\n",
    "# Yeni sütun: 'percentage' (yüzde)\n",
    "df_single_freq['percentage'] = (df_single_freq['count'] / total_count_single) * 100\n",
    "df_single_percent = df_single_freq[['word', 'count', 'percentage', 'z_score', 'z_score_log', 'z_categories']]\n",
    "\n",
    "# 9) Sırala ve sonucu yazdır\n",
    "df_single_freq = df_single_freq.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "#print(df_single_freq)\n",
    "df_single_percent = df_single_percent.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "print(f\"Ortalama: {mean_val_single}\")\n",
    "print(f\"Standart sapma: {std_val_single}\")\n",
    "#print(df_single_percent)\n",
    "# Çok düşük olanları sırala\n",
    "print(df_single_percent[df_single_percent['z_categories'] == 'orta düşük'])\n",
    "#print(df_single_percent[df_single_percent['word'] == 'büyük'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# İki Harfli Kısaltmalar: Z-Skoru\n",
    "\n",
    "# CSV'yi oku\n",
    "csv_path = 'data/contractions/word_frequencies.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# İki harfli kısaltmalar listesi\n",
    "csv_path_con_double = 'data/contractions/con_double.csv'\n",
    "df_con_double = pd.read_csv(csv_path_con_double, header=None)\n",
    "# 2. sütundaki değerleri listeye al\n",
    "contractions_double = df_con_double[1].tolist()\n",
    "print(contractions_double)\n",
    "\n",
    "# 1) Listedeki her sözcükle BAŞLAYAN tüm sözcüklerin toplam frekansını bulalım\n",
    "results = []\n",
    "for cword in contractions_double:\n",
    "    sub_count = df[df['word'].str.startswith(cword, na=False)]['count'].sum()\n",
    "    # Ünsüz yumuşaması varsa frekansa ekle\n",
    "    if cword == \"cevap\":\n",
    "        sub_count += df[df['word'].str.startswith(\"cevab\", na=False)]['count'].sum()\n",
    "    elif cword == \"çeşit\":\n",
    "        sub_count += df[df['word'].str.startswith(\"çeşid\", na=False)]['count'].sum()\n",
    "    elif cword == \"çocuk\":\n",
    "        sub_count += df[df['word'].str.startswith(\"çocuğ\", na=False)]['count'].sum()\n",
    "    elif cword == \"kitap\":\n",
    "        sub_count += df[df['word'].str.startswith(\"kitab\", na=False)]['count'].sum()\n",
    "    elif cword == \"küçük\":\n",
    "        sub_count += df[df['word'].str.startswith(\"küçüğ\", na=False)]['count'].sum()\n",
    "    elif cword == \"sebep\":\n",
    "        sub_count += df[df['word'].str.startswith(\"sebeb\", na=False)]['count'].sum()\n",
    "    elif cword == \"sıcak\":\n",
    "        sub_count += df[df['word'].str.startswith(\"sıcağ\", na=False)]['count'].sum()\n",
    "    elif cword == \"soğuk\":\n",
    "        sub_count += df[df['word'].str.startswith(\"soğuğ\", na=False)]['count'].sum()\n",
    "    elif cword == \"toprak\":\n",
    "        sub_count += df[df['word'].str.startswith(\"toprağ\", na=False)]['count'].sum()\n",
    "    # Ünlü düşmesi varsa frekansa ekle\n",
    "    elif cword == \"cisim\":\n",
    "        sub_count += df[df['word'].str.startswith(\"cism\", na=False)]['count'].sum()\n",
    "    elif cword == \"kısım\":\n",
    "        sub_count += df[df['word'].str.startswith(\"kısm\", na=False)]['count'].sum()\n",
    "\n",
    "    results.append({'word': cword, 'count': sub_count})\n",
    "\n",
    "df_double_freq = pd.DataFrame(results)\n",
    "\n",
    "# 2) Ortalamayı ve standart sapmayı hesapla\n",
    "mean_val_double = df_double_freq['count'].mean()\n",
    "std_val_double = df_double_freq['count'].std()\n",
    "\n",
    "# 3) Standart sapmanın 0 olması ihtimaline karşı kontrol yap\n",
    "if std_val_double == 0:\n",
    "    df_double_freq['z_score'] = 0\n",
    "else:\n",
    "    # (count - ortalama) / std formülüyle z-skorunu ekleyelim\n",
    "\n",
    "    df_double_freq['z_score'] = (df_double_freq['count'] - mean_val_double) / std_val_double\n",
    "\n",
    "# 4) Log-Transfer uygula\n",
    "# 'count' değerleri için log(1+x) dönüşümü\n",
    "df_double_freq['log_count'] = np.log1p(df_double_freq['count'])  # log(count + 1)\n",
    "\n",
    "# 5) Log değerlerin ortalamasını ve std'sini alın\n",
    "mean_double_log = df_double_freq['log_count'].mean()\n",
    "std_double_log = df_double_freq['log_count'].std()\n",
    "\n",
    "# 6) Log değer üzerinden z-skoru\n",
    "if std_double_log == 0:\n",
    "    df_double_freq['z_score_log'] = 0\n",
    "else:\n",
    "    df_double_freq['z_score_log'] = (df_double_freq['log_count'] - mean_double_log) / std_double_log\n",
    "\n",
    "# 7) Z-Skoruna göre kategorilere ayır\n",
    "# Kategoriler için bin aralıkları (soldan sağa sıralı):\n",
    "bins = [-np.inf, -1, -0.5, 0, 0.5, 1, np.inf]\n",
    "# Bu aralıklara denk gelen etiketler (labels):\n",
    "labels = [\"çok düşük\", \"düşük\", \"orta düşük\", \"orta yüksek\", \"yüksek\", \"çok yüksek\"]\n",
    "df_double_freq['z_categories'] = pd.cut(df_double_freq['z_score_log'], bins=bins, labels=labels)\n",
    "\n",
    "# 8) Yüzdelik hesapla\n",
    "# Toplam 'count' değerini hesapla\n",
    "total_count_double = df_double_freq['count'].sum()\n",
    "# Yeni sütun: 'percentage' (yüzde)\n",
    "df_double_freq['percentage'] = (df_double_freq['count'] / total_count_double) * 100\n",
    "df_double_percent = df_double_freq[['word', 'count', 'percentage', 'z_score_log', 'z_categories']]\n",
    "\n",
    "\n",
    "# 9) Sırala ve sonucu yazdır\n",
    "df_double_percent = df_double_percent.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "print(mean_val_double)\n",
    "print(std_val_double)\n",
    "#print(df_double_percent)\n",
    "# Çok düşük olanları sırala\n",
    "print(df_double_percent[df_double_percent['z_categories'] == 'çok yüksek'])\n",
    "#print(df_double_percent[df_double_percent['word'] == 'küçük'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
